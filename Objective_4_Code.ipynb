{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95COnxY_ivYJ",
        "outputId": "2a7a5ee0-dde2-4214-953b-ee83cca44291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-P9lQvee9L2",
        "outputId": "66d617f5-8909-4780-edff-a188e9e3c919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data has been saved to /content/drive/MyDrive/M.S. BUSINESS ANALYTICS/MIS612/CO-OP SURVEY GROUP PROJECT/Objective 2/Objective_2_Findings.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# File paths\n",
        "input_file_path = \"/content/drive/MyDrive/M.S. BUSINESS ANALYTICS/MIS612/CO-OP SURVEY GROUP PROJECT/Objective 2/newwork file.xlsx\"  # Input file path\n",
        "output_file_path = \"/content/drive/MyDrive/M.S. BUSINESS ANALYTICS/MIS612/CO-OP SURVEY GROUP PROJECT/Objective 2/Objective_2_Findings.xlsx\"  # Output file path\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Enhanced improvement categories with expanded synonyms\n",
        "improvement_categories = {\n",
        "    'In_Person_Work': [\n",
        "        'person', 'office', 'remote', 'hybrid', 'physical', 'location',\n",
        "        'site', 'building', 'workplace', 'onsite', 'facility', 'face',\n",
        "        'virtual', 'inperson', 'workspace', 'presence', 'commute',\n",
        "        'premises', 'campus', 'branch', 'offline', 'online', 'distance',\n",
        "        'work from home', 'wfh', 'desk', 'cubicle', 'headquarter', 'hq',\n",
        "        'return to office', 'rto', 'in-office', 'face to face', 'face-to-face'\n",
        "    ],\n",
        "    'Project_Tasks': [\n",
        "        'project', 'task', 'assignment', 'work', 'responsibility',\n",
        "        'duty', 'role', 'job', 'workload', 'busy', 'assignment',\n",
        "        'duties', 'projects', 'tasks', 'variety', 'activities',\n",
        "        'opportunities', 'more work', 'additional', 'new',\n",
        "        'hands on', 'hands-on', 'deliverable', 'action', 'objective',\n",
        "        'goal', 'initiative', 'mission', 'operation', 'function',\n",
        "        'undertaking', 'engagement', 'campaign', 'program', 'venture',\n",
        "        'meaningful work', 'substantial', 'significant', 'important'\n",
        "    ],\n",
        "    'Team_Interaction': [\n",
        "        'team', 'people', 'coworker', 'colleague', 'staff', 'member',\n",
        "        'group', 'others', 'peer', 'social', 'interaction', 'together',\n",
        "        'collaborative', 'collaboration', 'communicate', 'interact',\n",
        "        'coworkers', 'colleagues', 'teammates', 'cooperation',\n",
        "        'network', 'networking', 'bond', 'connect', 'relationship',\n",
        "        'community', 'fellowship', 'partnership', 'alliance', 'crew',\n",
        "        'squad', 'unit', 'coordination', 'synergy', 'age', 'younger',\n",
        "        'older', 'social interaction', 'lunch', 'events', 'gathering'\n",
        "    ],\n",
        "    'Learning_Development': [\n",
        "        'learning', 'training', 'development', 'learn', 'skill',\n",
        "        'knowledge', 'education', 'growth', 'experience', 'mentor',\n",
        "        'teaching', 'guidance', 'learn', 'develop', 'improve',\n",
        "        'understand', 'exposure', 'opportunity', 'career',\n",
        "        'workshop', 'seminar', 'course', 'certification', 'study',\n",
        "        'practice', 'instruction', 'coaching', 'tutoring', 'shadowing',\n",
        "        'apprenticeship', 'internship', 'professional development',\n",
        "        'skill building', 'expertise', 'competency', 'capability',\n",
        "        'qualification', 'advancement', 'progress', 'evolve'\n",
        "    ],\n",
        "    'Nothing': [\n",
        "        'nothing', 'none', 'na', 'n/a', 'already', 'perfect', 'great',\n",
        "        'satisfied', 'enjoyed', 'loved', 'fantastic', 'excellent',\n",
        "        'good', 'fine', 'ok', 'okay', 'sufficient', 'adequate',\n",
        "        'complete', 'thorough', 'comprehensive', 'enough', 'satisfied',\n",
        "        'happy', 'content', 'pleased', 'no', 'not', 'cant think'\n",
        "    ],\n",
        "    'Other': []  # Will be calculated based on no matches in other categories\n",
        "}\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text to clean and prepare for analysis.\"\"\"\n",
        "    if pd.isna(text) or text.lower() in ['na', 'n/a', '-', 'nothing', 'none', 'idk', 'err:509', '#name?']:\n",
        "        return ''\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join(lemmatizer.lemmatize(t) for t in tokens)\n",
        "\n",
        "def analyze_text(response):\n",
        "    \"\"\"Analyze the preprocessed text and classify into binary categories.\"\"\"\n",
        "    analysis = {category: int(any(k in response for k in keywords))\n",
        "                for category, keywords in improvement_categories.items() if category != 'Other'}\n",
        "    # Add the \"Other\" column: 1 if all categories are 0\n",
        "    analysis['Other'] = int(all(value == 0 for value in analysis.values()))\n",
        "    return pd.Series(analysis)\n",
        "\n",
        "def analyze_responses(file_path, skip_rows=2):\n",
        "    \"\"\"Main function to analyze responses from an Excel file.\"\"\"\n",
        "    # Load the Excel file\n",
        "    df = pd.read_excel(file_path, skiprows=skip_rows)\n",
        "\n",
        "    # Set the correct column names\n",
        "    df.columns = [\n",
        "        'Responder ID',\n",
        "        'Terms',\n",
        "        'Coll',\n",
        "        'Major',\n",
        "        'Conc',\n",
        "        'Class BOT',\n",
        "        'Citizenship',\n",
        "        'Coop #',\n",
        "        'What would have made this job more interesting to you?'\n",
        "    ]\n",
        "\n",
        "    # Preprocess responses\n",
        "    processed_responses = df['What would have made this job more interesting to you?'].apply(preprocess_text)\n",
        "\n",
        "    # Analyze text and create binary analysis matrix\n",
        "    binary_df = processed_responses.apply(analyze_text)\n",
        "\n",
        "    # Combine original DataFrame with binary analysis results (exclude processed responses)\n",
        "    result_df = pd.concat([df.drop(columns=['What would have made this job more interesting to you?']), binary_df], axis=1)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Analyze responses\n",
        "    result_df = analyze_responses(input_file_path)\n",
        "\n",
        "    # Save results (binary columns only)\n",
        "    result_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"\\nData has been saved to {output_file_path}\")\n"
      ]
    }
  ]
}